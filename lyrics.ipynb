{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Lambda, LSTM, Dense, Dropout, Input, Bidirectional\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import simple_preprocess\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "from numba import cuda "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Austin\\AppData\\Local\\Temp\\ipykernel_17892\\1897261821.py:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "True\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.test.is_gpu_available())\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_GPU():\n",
    "    device = cuda.get_current_device()\n",
    "    device.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE_PDUMP = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading from lyrics_l.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_fil_df = pd.read_pickle('pickles/fin.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE_PDUMP:\n",
    "    directory_path = 'data/music4all_subset/lyrics'\n",
    "    file_prefixes = fin_fil_df['id'].tolist()\n",
    "    lyrics_d = {}\n",
    "    for prefix in file_prefixes:\n",
    "        file_pattern = prefix + '.txt'\n",
    "        file_path = os.path.join(directory_path, file_pattern)\n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path, 'r', encoding=\"utf8\") as file:\n",
    "                content = file.read()\n",
    "                lyrics_d[prefix] = content\n",
    "                lyrics_l.append(content)\n",
    "                # print(f\"Content of {file_pattern}:\\n{content}\")\n",
    "    \n",
    "    with open('pickles/lyrics_l.pkl', 'wb') as f:\n",
    "        pickle.dump(lyrics_l, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pickles/lyrics_l.pkl', 'rb') as f:\n",
    "    lyrics_l = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_lyrics(lyli):\n",
    "    def prep_single(ly):\n",
    "        text = re.sub(r'[^\\w\\s]', '', ly).lower()\n",
    "        text = text.replace('\\n', ' ')\n",
    "        text = re.sub(' +', ' ', text)\n",
    "        return text\n",
    "    new = [prep_single(l) for l in lyli]\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_lyrics = preprocess_lyrics(lyrics_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alternative rock' 'ambient' 'classic rock' 'electronic' 'folk'\n",
      " 'indie rock' 'metal' 'pop' 'rap' 'singer-songwriter' 'soul']\n"
     ]
    }
   ],
   "source": [
    "encoder = LabelEncoder()\n",
    "labels = fin_fil_df['genre']\n",
    "genre_labels = encoder.fit_transform(labels)\n",
    "print(encoder.classes_)\n",
    "X_train, X_test, y_train, y_test = train_test_split(prep_lyrics, genre_labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE_PDUMP:\n",
    "    with open('pickles/lyrics_ttsdata.pkl', 'wb') as f:\n",
    "        pickle.dump((X_train, X_test, y_train, y_test), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lyric Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_cb = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
    "def getModelCheckpoint(name):\n",
    "    return ModelCheckpoint(\n",
    "        filepath=f'models/{name}.h5',\n",
    "        save_best_only=True,\n",
    "        monitor='val_accuracy',\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELMo with Dense Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo = hub.load('https://tfhub.dev/google/elmo/3').signatures['default']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elmo_vectors(x):\n",
    "    embeddings = elmo(tf.constant(x))[\"elmo\"]\n",
    "    return tf.reduce_mean(embeddings, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE_PDUMP:\n",
    "    list_train = [X_train[i:i + 100] for i in range(0, len(X_train), 100)]\n",
    "    list_test = [X_test[i:i + 100] for i in range(0, len(X_test), 100)]\n",
    "    elmo_train = [elmo_vectors(x) for x in list_train]\n",
    "    elmo_test = [elmo_vectors(x) for x in list_test]\n",
    "    elmo_train_new = np.concatenate(elmo_train, axis = 0)\n",
    "    elmo_test_new = np.concatenate(elmo_test, axis = 0)\n",
    "\n",
    "    with tf.device('/GPU:0'):\n",
    "        gpu_lyrics = tf.constant(prep_lyrics)\n",
    "        embeddings = elmo(gpu_lyrics)['elmo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not MODE_PDUMP:\n",
    "    with open('pickles/elmo0.pkl', 'rb') as f:\n",
    "        e0 = pickle.load(f)\n",
    "    with open('pickles/elmo1.pkl', 'rb') as f:\n",
    "        e1 = pickle.load(f)\n",
    "    with open('pickles/elmo2.pkl', 'rb') as f:\n",
    "        e2 = pickle.load(f)\n",
    "    with open('pickles/elmo3.pkl', 'rb') as f:\n",
    "        e3 = pickle.load(f)\n",
    "    with open('pickles/elmo_test.pkl', 'rb') as f:\n",
    "        e_test = pickle.load(f)\n",
    "    e0 = tf.concat(e0, axis=0)\n",
    "    e1 = tf.concat(e1, axis=0)\n",
    "    e2 = tf.concat(e2, axis=0)\n",
    "    e3 = tf.concat(e3, axis=0)\n",
    "    e_test = tf.concat(e_test, axis=0)\n",
    "    e_train = tf.concat([e0, e1, e2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(26850, 1024), dtype=float32, numpy=\n",
       "array([[ 0.10733027, -0.13069776, -0.00867425, ..., -0.1278605 ,\n",
       "         0.45776153, -0.02518587],\n",
       "       [-0.04765201, -0.1638587 ,  0.05751887, ...,  0.04533917,\n",
       "         0.27689737,  0.03030533],\n",
       "       [ 0.07918552, -0.1052879 ,  0.09017083, ...,  0.01571165,\n",
       "         0.35622203, -0.05648162],\n",
       "       ...,\n",
       "       [-0.03044983, -0.06164612,  0.03256677, ...,  0.03354283,\n",
       "         0.03826979, -0.0157944 ],\n",
       "       [ 0.11266568, -0.17386876,  0.07004549, ...,  0.0156843 ,\n",
       "         0.36772186,  0.00108926],\n",
       "       [-0.00505059, -0.0863753 ,  0.07504102, ...,  0.05298683,\n",
       "         0.13353385, -0.05152311]], dtype=float32)>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 11\n",
    "\n",
    "elmo_dense = tf.keras.Sequential([\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dropout(.5),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(.5),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "chosen_opt ='adam'\n",
    "elmo_dense.compile(loss='sparse_categorical_crossentropy', optimizer=chosen_opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "833/840 [============================>.] - ETA: 0s - loss: 2.2168 - accuracy: 0.2746\n",
      "Epoch 1: val_accuracy improved from -inf to 0.27727, saving model to models\\elmo_dense.h5\n",
      "840/840 [==============================] - 4s 4ms/step - loss: 2.2169 - accuracy: 0.2746 - val_loss: 2.2006 - val_accuracy: 0.2773\n",
      "Epoch 2/10\n",
      "824/840 [============================>.] - ETA: 0s - loss: 2.2007 - accuracy: 0.2762\n",
      "Epoch 2: val_accuracy did not improve from 0.27727\n",
      "840/840 [==============================] - 3s 4ms/step - loss: 2.2007 - accuracy: 0.2763 - val_loss: 2.1931 - val_accuracy: 0.2773\n",
      "Epoch 3/10\n",
      "840/840 [==============================] - ETA: 0s - loss: 2.1950 - accuracy: 0.2762\n",
      "Epoch 3: val_accuracy did not improve from 0.27727\n",
      "840/840 [==============================] - 3s 4ms/step - loss: 2.1950 - accuracy: 0.2762 - val_loss: 2.1930 - val_accuracy: 0.2773\n",
      "Epoch 4/10\n",
      "828/840 [============================>.] - ETA: 0s - loss: 2.1952 - accuracy: 0.2760\n",
      "Epoch 4: val_accuracy did not improve from 0.27727\n",
      "840/840 [==============================] - 3s 4ms/step - loss: 2.1953 - accuracy: 0.2762 - val_loss: 2.1903 - val_accuracy: 0.2773\n",
      "Epoch 5/10\n",
      "836/840 [============================>.] - ETA: 0s - loss: 2.1941 - accuracy: 0.2764\n",
      "Epoch 5: val_accuracy did not improve from 0.27727\n",
      "840/840 [==============================] - 3s 4ms/step - loss: 2.1941 - accuracy: 0.2762 - val_loss: 2.1897 - val_accuracy: 0.2773\n",
      "Epoch 6/10\n",
      "834/840 [============================>.] - ETA: 0s - loss: 2.1945 - accuracy: 0.2760\n",
      "Epoch 6: val_accuracy did not improve from 0.27727\n",
      "840/840 [==============================] - 3s 4ms/step - loss: 2.1940 - accuracy: 0.2762 - val_loss: 2.1894 - val_accuracy: 0.2773\n",
      "Epoch 7/10\n",
      "828/840 [============================>.] - ETA: 0s - loss: 2.1936 - accuracy: 0.2761\n",
      "Epoch 7: val_accuracy did not improve from 0.27727\n",
      "840/840 [==============================] - 3s 4ms/step - loss: 2.1935 - accuracy: 0.2762 - val_loss: 2.1913 - val_accuracy: 0.2773\n",
      "Epoch 8/10\n",
      "835/840 [============================>.] - ETA: 0s - loss: 2.1934 - accuracy: 0.2760\n",
      "Epoch 8: val_accuracy did not improve from 0.27727\n",
      "840/840 [==============================] - 3s 4ms/step - loss: 2.1933 - accuracy: 0.2762 - val_loss: 2.1909 - val_accuracy: 0.2773\n",
      "Epoch 9/10\n",
      "839/840 [============================>.] - ETA: 0s - loss: 2.1935 - accuracy: 0.2762\n",
      "Epoch 9: val_accuracy did not improve from 0.27727\n",
      "840/840 [==============================] - 3s 4ms/step - loss: 2.1935 - accuracy: 0.2762 - val_loss: 2.1894 - val_accuracy: 0.2773\n",
      "Epoch 10/10\n",
      "829/840 [============================>.] - ETA: 0s - loss: 2.1925 - accuracy: 0.2764\n",
      "Epoch 10: val_accuracy did not improve from 0.27727\n",
      "840/840 [==============================] - 3s 4ms/step - loss: 2.1927 - accuracy: 0.2762 - val_loss: 2.1918 - val_accuracy: 0.2773\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2973e12d910>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elmo_dense.fit(e_train, y_train[:26850], validation_data=(e_test, y_train[26850:]), epochs=10, batch_size=32,\n",
    "             callbacks=[es_cb, getModelCheckpoint(\"elmo_dense\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec with Dense Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_sen = [simple_preprocess(s) for s in X_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(194474675, 265219020)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v = Word2Vec(p_sen, vector_size=1000, window=20, min_count=5, sg=0)\n",
    "w2v.train(p_sen, total_examples=len(p_sen), epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_emb_train = []\n",
    "for s in X_train:\n",
    "    words = simple_preprocess(s)\n",
    "    wb = []\n",
    "    for word in words:\n",
    "        if word in w2v.wv:\n",
    "            wb.append(w2v.wv[word])\n",
    "    w2v_emb_train.append(np.mean(wb, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_emb_test = []\n",
    "for s in X_test:\n",
    "    words = simple_preprocess(s)\n",
    "    wb = []\n",
    "    for word in words:\n",
    "        if word in w2v.wv:\n",
    "            wb.append(w2v.wv[word])\n",
    "    w2v_emb_test.append(np.mean(wb, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_train = np.vstack(w2v_emb_train)\n",
    "w2v_test = np.vstack(w2v_emb_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 11\n",
    "\n",
    "w2v_dense = tf.keras.Sequential([\n",
    "    Dense(1024, activation='relu', input_dim=w2v.vector_size),\n",
    "    Dropout(.5),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(.5),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "chosen_opt ='adam'\n",
    "w2v_dense.compile(loss='sparse_categorical_crossentropy', optimizer=chosen_opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1116/1122 [============================>.] - ETA: 0s - loss: 1.9196 - accuracy: 0.3500\n",
      "Epoch 1: val_accuracy improved from -inf to 0.38498, saving model to models\\w2v_dense.h5\n",
      "1122/1122 [==============================] - 5s 4ms/step - loss: 1.9193 - accuracy: 0.3502 - val_loss: 1.8024 - val_accuracy: 0.3850\n",
      "Epoch 2/30\n",
      "1108/1122 [============================>.] - ETA: 0s - loss: 1.8295 - accuracy: 0.3742\n",
      "Epoch 2: val_accuracy improved from 0.38498 to 0.39813, saving model to models\\w2v_dense.h5\n",
      "1122/1122 [==============================] - 4s 4ms/step - loss: 1.8305 - accuracy: 0.3743 - val_loss: 1.7940 - val_accuracy: 0.3981\n",
      "Epoch 3/30\n",
      "1111/1122 [============================>.] - ETA: 0s - loss: 1.7866 - accuracy: 0.3901\n",
      "Epoch 3: val_accuracy improved from 0.39813 to 0.40002, saving model to models\\w2v_dense.h5\n",
      "1122/1122 [==============================] - 4s 4ms/step - loss: 1.7872 - accuracy: 0.3897 - val_loss: 1.7581 - val_accuracy: 0.4000\n",
      "Epoch 4/30\n",
      "1119/1122 [============================>.] - ETA: 0s - loss: 1.7593 - accuracy: 0.3985\n",
      "Epoch 4: val_accuracy did not improve from 0.40002\n",
      "1122/1122 [==============================] - 4s 4ms/step - loss: 1.7595 - accuracy: 0.3984 - val_loss: 1.7519 - val_accuracy: 0.3975\n",
      "Epoch 5/30\n",
      "1118/1122 [============================>.] - ETA: 0s - loss: 1.7398 - accuracy: 0.4032\n",
      "Epoch 5: val_accuracy improved from 0.40002 to 0.40169, saving model to models\\w2v_dense.h5\n",
      "1122/1122 [==============================] - 4s 4ms/step - loss: 1.7398 - accuracy: 0.4031 - val_loss: 1.7377 - val_accuracy: 0.4017\n",
      "Epoch 6/30\n",
      "1108/1122 [============================>.] - ETA: 0s - loss: 1.7164 - accuracy: 0.4092\n",
      "Epoch 6: val_accuracy improved from 0.40169 to 0.40994, saving model to models\\w2v_dense.h5\n",
      "1122/1122 [==============================] - 4s 4ms/step - loss: 1.7164 - accuracy: 0.4089 - val_loss: 1.7252 - val_accuracy: 0.4099\n",
      "Epoch 7/30\n",
      "1112/1122 [============================>.] - ETA: 0s - loss: 1.6931 - accuracy: 0.4151\n",
      "Epoch 7: val_accuracy improved from 0.40994 to 0.41117, saving model to models\\w2v_dense.h5\n",
      "1122/1122 [==============================] - 4s 4ms/step - loss: 1.6935 - accuracy: 0.4152 - val_loss: 1.7257 - val_accuracy: 0.4112\n",
      "Epoch 8/30\n",
      "1107/1122 [============================>.] - ETA: 0s - loss: 1.6724 - accuracy: 0.4228\n",
      "Epoch 8: val_accuracy did not improve from 0.41117\n",
      "1122/1122 [==============================] - 4s 4ms/step - loss: 1.6723 - accuracy: 0.4229 - val_loss: 1.7325 - val_accuracy: 0.4077\n",
      "Epoch 9/30\n",
      "1110/1122 [============================>.] - ETA: 0s - loss: 1.6509 - accuracy: 0.4311\n",
      "Epoch 9: val_accuracy improved from 0.41117 to 0.41195, saving model to models\\w2v_dense.h5\n",
      "1122/1122 [==============================] - 4s 4ms/step - loss: 1.6515 - accuracy: 0.4309 - val_loss: 1.7205 - val_accuracy: 0.4119\n",
      "Epoch 10/30\n",
      "1116/1122 [============================>.] - ETA: 0s - loss: 1.6291 - accuracy: 0.4372\n",
      "Epoch 10: val_accuracy did not improve from 0.41195\n",
      "1122/1122 [==============================] - 4s 4ms/step - loss: 1.6294 - accuracy: 0.4371 - val_loss: 1.7396 - val_accuracy: 0.4021\n",
      "Epoch 11/30\n",
      "1115/1122 [============================>.] - ETA: 0s - loss: 1.6072 - accuracy: 0.4424\n",
      "Epoch 11: val_accuracy did not improve from 0.41195\n",
      "1122/1122 [==============================] - 4s 4ms/step - loss: 1.6071 - accuracy: 0.4423 - val_loss: 1.7294 - val_accuracy: 0.4109\n",
      "Epoch 12/30\n",
      "1114/1122 [============================>.] - ETA: 0s - loss: 1.5846 - accuracy: 0.4539\n",
      "Epoch 12: val_accuracy did not improve from 0.41195\n",
      "1122/1122 [==============================] - 4s 4ms/step - loss: 1.5846 - accuracy: 0.4539 - val_loss: 1.7393 - val_accuracy: 0.4087\n",
      "Epoch 12: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29b04f9d760>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_dense.fit(w2v_train, y_train, validation_data=(w2v_test, y_test), epochs=30, batch_size=32,\n",
    "             callbacks=[es_cb, getModelCheckpoint(\"w2v_dense\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_60 (Dense)            (None, 1024)              1025024   \n",
      "                                                                 \n",
      " dropout_44 (Dropout)        (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_61 (Dense)            (None, 512)               524800    \n",
      "                                                                 \n",
      " dropout_45 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_62 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_63 (Dense)            (None, 11)                2827      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,683,979\n",
      "Trainable params: 1,683,979\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "w2v_dense.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe with Dense Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dict = {}\n",
    "with open('glove/glove.6B/glove.6B.300d.txt','r', encoding=\"utf-8\") as f:\n",
    "  for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    vector = np.asarray(values[1:],'float32')\n",
    "    embed_dict[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_emb_train = []\n",
    "for s in X_train:\n",
    "    words = s.split()\n",
    "    wb = []\n",
    "    for word in words:\n",
    "        if word in embed_dict:\n",
    "            wb.append(embed_dict[word])\n",
    "    glove_emb_train.append(np.mean(wb, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_emb_test = []\n",
    "for s in X_test:\n",
    "    words = s.split()\n",
    "    wb = []\n",
    "    for word in words:\n",
    "        if word in embed_dict:\n",
    "            wb.append(embed_dict[word])\n",
    "    glove_emb_test.append(np.mean(wb, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_train = np.vstack(glove_emb_train)\n",
    "glove_test = np.vstack(glove_emb_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 11\n",
    "\n",
    "glove_dense = tf.keras.Sequential([\n",
    "    Dense(1024, activation='relu', input_dim=300),\n",
    "    Dropout(.5),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(.5),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "chosen_opt ='adam'\n",
    "glove_dense.compile(loss='sparse_categorical_crossentropy', optimizer=chosen_opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1117/1122 [============================>.] - ETA: 0s - loss: 2.0527 - accuracy: 0.3110\n",
      "Epoch 1: val_accuracy improved from -inf to 0.35366, saving model to models\\glove_dense.h5\n",
      "1122/1122 [==============================] - 4s 4ms/step - loss: 2.0525 - accuracy: 0.3110 - val_loss: 1.9422 - val_accuracy: 0.3537\n",
      "Epoch 2/30\n",
      "1108/1122 [============================>.] - ETA: 0s - loss: 1.9530 - accuracy: 0.3403\n",
      "Epoch 2: val_accuracy improved from 0.35366 to 0.35923, saving model to models\\glove_dense.h5\n",
      "1122/1122 [==============================] - 4s 4ms/step - loss: 1.9537 - accuracy: 0.3398 - val_loss: 1.8848 - val_accuracy: 0.3592\n",
      "Epoch 3/30\n",
      "1113/1122 [============================>.] - ETA: 0s - loss: 1.9220 - accuracy: 0.3468\n",
      "Epoch 3: val_accuracy improved from 0.35923 to 0.37338, saving model to models\\glove_dense.h5\n",
      "1122/1122 [==============================] - 4s 4ms/step - loss: 1.9216 - accuracy: 0.3470 - val_loss: 1.8614 - val_accuracy: 0.3734\n",
      "Epoch 4/30\n",
      "1110/1122 [============================>.] - ETA: 0s - loss: 1.9062 - accuracy: 0.3503\n",
      "Epoch 4: val_accuracy improved from 0.37338 to 0.37394, saving model to models\\glove_dense.h5\n",
      "1122/1122 [==============================] - 4s 4ms/step - loss: 1.9054 - accuracy: 0.3506 - val_loss: 1.8398 - val_accuracy: 0.3739\n",
      "Epoch 5/30\n",
      "1111/1122 [============================>.] - ETA: 0s - loss: 1.8888 - accuracy: 0.3572\n",
      "Epoch 5: val_accuracy did not improve from 0.37394\n",
      "1122/1122 [==============================] - 4s 4ms/step - loss: 1.8890 - accuracy: 0.3570 - val_loss: 1.8723 - val_accuracy: 0.3657\n",
      "Epoch 6/30\n",
      "1112/1122 [============================>.] - ETA: 0s - loss: 1.8824 - accuracy: 0.3597\n",
      "Epoch 6: val_accuracy improved from 0.37394 to 0.37963, saving model to models\\glove_dense.h5\n",
      "1122/1122 [==============================] - 4s 4ms/step - loss: 1.8825 - accuracy: 0.3595 - val_loss: 1.8329 - val_accuracy: 0.3796\n",
      "Epoch 7/30\n",
      "1120/1122 [============================>.] - ETA: 0s - loss: 1.8687 - accuracy: 0.3645\n",
      "Epoch 7: val_accuracy did not improve from 0.37963\n",
      "1122/1122 [==============================] - 4s 4ms/step - loss: 1.8682 - accuracy: 0.3647 - val_loss: 1.8245 - val_accuracy: 0.3756\n",
      "Epoch 8/30\n",
      "1114/1122 [============================>.] - ETA: 0s - loss: 1.8620 - accuracy: 0.3673\n",
      "Epoch 8: val_accuracy improved from 0.37963 to 0.38085, saving model to models\\glove_dense.h5\n",
      "1122/1122 [==============================] - 4s 4ms/step - loss: 1.8623 - accuracy: 0.3671 - val_loss: 1.8158 - val_accuracy: 0.3809\n",
      "Epoch 9/30\n",
      "1116/1122 [============================>.] - ETA: 0s - loss: 1.8565 - accuracy: 0.3661\n",
      "Epoch 9: val_accuracy did not improve from 0.38085\n",
      "1122/1122 [==============================] - 4s 4ms/step - loss: 1.8561 - accuracy: 0.3662 - val_loss: 1.8287 - val_accuracy: 0.3723\n",
      "Epoch 10/30\n",
      "1107/1122 [============================>.] - ETA: 0s - loss: 1.8541 - accuracy: 0.3652\n",
      "Epoch 10: val_accuracy did not improve from 0.38085\n",
      "1122/1122 [==============================] - 4s 4ms/step - loss: 1.8536 - accuracy: 0.3655 - val_loss: 1.8252 - val_accuracy: 0.3797\n",
      "Epoch 11/30\n",
      "1118/1122 [============================>.] - ETA: 0s - loss: 1.8458 - accuracy: 0.3693\n",
      "Epoch 11: val_accuracy did not improve from 0.38085\n",
      "1122/1122 [==============================] - 4s 4ms/step - loss: 1.8457 - accuracy: 0.3692 - val_loss: 1.8170 - val_accuracy: 0.3781\n",
      "Epoch 11: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29b4e6d40a0>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_dense.fit(glove_train, y_train, validation_data=(glove_test, y_test), epochs=30, batch_size=32,\n",
    "             callbacks=[es_cb, getModelCheckpoint(\"glove_dense\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Tokenizer with Dense Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (822 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35888, 5184)\n"
     ]
    }
   ],
   "source": [
    "if MODE_PDUMP:\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    \n",
    "    train_tokenized_sentences = tokenizer(X_train, padding=True, return_tensors=\"tf\")\n",
    "    test_tokenized_sentences = tokenizer(X_test, padding=True, return_tensors=\"tf\")\n",
    "    \n",
    "    print(train_tokenized_sentences['input_ids'].shape)\n",
    "    \n",
    "    train_tokenized_sentences = train_tokenized_sentences['input_ids']\n",
    "    test_tokenized_sentences = test_tokenized_sentences['input_ids']\n",
    "    with open('pickles/bert_tok_train.pkl', 'wb') as f:\n",
    "        pickle.dump(train_tokenized_sentences, f)\n",
    "    with open('pickles/bert_tok_test.pkl', 'wb') as f:\n",
    "        pickle.dump(test_tokenized_sentences, f)\n",
    "else:\n",
    "    with open('pickles/bert_tok_train.pkl', 'rb') as f:\n",
    "        train_tokenized_sentences = pickle.load(f)\n",
    "    with open('pickles/bert_tok_test.pkl', 'rb') as f:\n",
    "        test_tokenized_sentences = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([8972, 2713])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_tokenized_sentences.shape\n",
    "test_tokenized_sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8972, 5184)\n",
      "2471\n"
     ]
    }
   ],
   "source": [
    "# Needed shape for the test tensor\n",
    "desired_shape = (test_tokenized_sentences.shape[0], train_tokenized_sentences.shape[1])\n",
    "print(desired_shape)\n",
    "num_columns_to_add = desired_shape[1] - test_tokenized_sentences.shape[1]\n",
    "print(num_columns_to_add)\n",
    "zeros_to_add = tf.zeros((desired_shape[0], num_columns_to_add), dtype=test_tokenized_sentences.dtype)\n",
    "resulting_tensor_x_test = tf.concat([test_tokenized_sentences, zeros_to_add], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(8972, 5184), dtype=int32, numpy=\n",
       "array([[  101,  1045, 10587, ...,     0,     0,     0],\n",
       "       [  101,  3280,  8814, ...,     0,     0,     0],\n",
       "       [  101,  2040,  2097, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [  101,  3564,  2006, ...,     0,     0,     0],\n",
       "       [  101,  2292,  2033, ...,     0,     0,     0],\n",
       "       [  101,  2129,  2079, ...,     0,     0,     0]])>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resulting_tensor_x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 11\n",
    "\n",
    "bert_dense = tf.keras.Sequential([\n",
    "    Dense(2048, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(.5),\n",
    "    Dense(1024, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(.5),\n",
    "    Dense(512, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(.5),\n",
    "    Dense(256, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(.5),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "chosen_opt = tf.keras.optimizers.Adam(1e-4)\n",
    "# chosen_opt ='adam'\n",
    "bert_dense.compile(loss='sparse_categorical_crossentropy', optimizer=chosen_opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1118/1122 [============================>.] - ETA: 0s - loss: 3.1362 - accuracy: 0.1501\n",
      "Epoch 1: val_accuracy improved from -inf to 0.30105, saving model to models\\bert_dense.h5\n",
      "1122/1122 [==============================] - 15s 12ms/step - loss: 3.1344 - accuracy: 0.1504 - val_loss: 2.2157 - val_accuracy: 0.3010\n",
      "Epoch 2/10\n",
      "1120/1122 [============================>.] - ETA: 0s - loss: 2.6010 - accuracy: 0.2156\n",
      "Epoch 2: val_accuracy improved from 0.30105 to 0.30261, saving model to models\\bert_dense.h5\n",
      "1122/1122 [==============================] - 13s 12ms/step - loss: 2.6013 - accuracy: 0.2156 - val_loss: 2.1643 - val_accuracy: 0.3026\n",
      "Epoch 3/10\n",
      "1120/1122 [============================>.] - ETA: 0s - loss: 2.3744 - accuracy: 0.2481\n",
      "Epoch 3: val_accuracy improved from 0.30261 to 0.30461, saving model to models\\bert_dense.h5\n",
      "1122/1122 [==============================] - 13s 12ms/step - loss: 2.3746 - accuracy: 0.2481 - val_loss: 2.1373 - val_accuracy: 0.3046\n",
      "Epoch 4/10\n",
      "1120/1122 [============================>.] - ETA: 0s - loss: 2.2694 - accuracy: 0.2696\n",
      "Epoch 4: val_accuracy improved from 0.30461 to 0.30885, saving model to models\\bert_dense.h5\n",
      "1122/1122 [==============================] - 13s 12ms/step - loss: 2.2694 - accuracy: 0.2696 - val_loss: 2.0966 - val_accuracy: 0.3088\n",
      "Epoch 5/10\n",
      "1121/1122 [============================>.] - ETA: 0s - loss: 2.1971 - accuracy: 0.2831\n",
      "Epoch 5: val_accuracy improved from 0.30885 to 0.31242, saving model to models\\bert_dense.h5\n",
      "1122/1122 [==============================] - 13s 12ms/step - loss: 2.1972 - accuracy: 0.2830 - val_loss: 2.0812 - val_accuracy: 0.3124\n",
      "Epoch 6/10\n",
      "1121/1122 [============================>.] - ETA: 0s - loss: 2.1582 - accuracy: 0.2911\n",
      "Epoch 6: val_accuracy did not improve from 0.31242\n",
      "1122/1122 [==============================] - 13s 11ms/step - loss: 2.1581 - accuracy: 0.2912 - val_loss: 2.0710 - val_accuracy: 0.3088\n",
      "Epoch 7/10\n",
      "1121/1122 [============================>.] - ETA: 0s - loss: 2.1242 - accuracy: 0.2937\n",
      "Epoch 7: val_accuracy did not improve from 0.31242\n",
      "1122/1122 [==============================] - 13s 11ms/step - loss: 2.1244 - accuracy: 0.2936 - val_loss: 2.0688 - val_accuracy: 0.3111\n",
      "Epoch 8/10\n",
      "1120/1122 [============================>.] - ETA: 0s - loss: 2.1042 - accuracy: 0.2988\n",
      "Epoch 8: val_accuracy did not improve from 0.31242\n",
      "1122/1122 [==============================] - 13s 11ms/step - loss: 2.1042 - accuracy: 0.2989 - val_loss: 2.0490 - val_accuracy: 0.3077\n",
      "Epoch 9/10\n",
      "1121/1122 [============================>.] - ETA: 0s - loss: 2.0841 - accuracy: 0.3019\n",
      "Epoch 9: val_accuracy improved from 0.31242 to 0.31587, saving model to models\\bert_dense.h5\n",
      "1122/1122 [==============================] - 13s 12ms/step - loss: 2.0843 - accuracy: 0.3019 - val_loss: 2.0517 - val_accuracy: 0.3159\n",
      "Epoch 10/10\n",
      "1120/1122 [============================>.] - ETA: 0s - loss: 2.0718 - accuracy: 0.3066\n",
      "Epoch 10: val_accuracy did not improve from 0.31587\n",
      "1122/1122 [==============================] - 13s 11ms/step - loss: 2.0718 - accuracy: 0.3067 - val_loss: 2.0429 - val_accuracy: 0.3096\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29933211dc0>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_dense.fit(train_tokenized_sentences, y_train, validation_data=(resulting_tensor_x_test, y_test), epochs=10, batch_size=32,\n",
    "               callbacks=[es_cb, getModelCheckpoint(\"bert_dense\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281/281 - 1s - loss: 2.0283 - accuracy: 0.3182 - 1s/epoch - 4ms/step\n",
      "Test Accuracy: 0.31821221113204956\n"
     ]
    }
   ],
   "source": [
    "l, a = bert_dense.evaluate(resulting_tensor_x_test, y_test, verbose=2)\n",
    "print(f'Test Accuracy: {a}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Network with Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tokenizer(oov_token='<UNK>')\n",
    "t.fit_on_texts(X_train)\n",
    "t.word_index['<PAD>'] = 0\n",
    "\n",
    "X_train_tok_lstm = t.texts_to_sequences(X_train)\n",
    "X_test_tok_lstm = t.texts_to_sequences(X_test)\n",
    "\n",
    "vocab_size = len(t.word_index)\n",
    "maxlen = len(max(X_train_tok_lstm, key=len))\n",
    "emb_dim = 300\n",
    "\n",
    "X_train_tok_lstm = sequence.pad_sequences(X_train_tok_lstm, maxlen=maxlen)\n",
    "X_test_tok_lstm = sequence.pad_sequences(X_test_tok_lstm, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 11\n",
    "\n",
    "tok_lstm = tf.keras.Sequential([\n",
    "    Embedding(vocab_size, emb_dim, input_length=maxlen),\n",
    "    LSTM(256, return_sequences=True, dropout=0.3),\n",
    "    Dropout(0.5),\n",
    "    LSTM(128, return_sequences=True, dropout=0.3),\n",
    "    Dropout(0.5),\n",
    "    LSTM(64, return_sequences=False, dropout=0.3),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# chosen_opt = tf.keras.optimizers.Adam(1e-4)\n",
    "chosen_opt ='adam'\n",
    "tok_lstm.compile(loss='sparse_categorical_crossentropy', optimizer=chosen_opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1122/1122 [==============================] - ETA: 0s - loss: 2.1719 - accuracy: 0.2793\n",
      "Epoch 1: val_accuracy improved from -inf to 0.27809, saving model to models\\tok_lstm.h5\n",
      "1122/1122 [==============================] - 888s 789ms/step - loss: 2.1719 - accuracy: 0.2793 - val_loss: 2.1894 - val_accuracy: 0.2781\n",
      "Epoch 2/10\n",
      "1122/1122 [==============================] - ETA: 0s - loss: 2.1931 - accuracy: 0.2776\n",
      "Epoch 2: val_accuracy did not improve from 0.27809\n",
      "1122/1122 [==============================] - 892s 795ms/step - loss: 2.1931 - accuracy: 0.2776 - val_loss: 2.1903 - val_accuracy: 0.2780\n",
      "Epoch 3/10\n",
      "1122/1122 [==============================] - ETA: 0s - loss: 2.1711 - accuracy: 0.2795\n",
      "Epoch 3: val_accuracy did not improve from 0.27809\n",
      "1122/1122 [==============================] - 905s 807ms/step - loss: 2.1711 - accuracy: 0.2795 - val_loss: 2.1187 - val_accuracy: 0.2754\n",
      "Epoch 4/10\n",
      "1122/1122 [==============================] - ETA: 0s - loss: 2.0676 - accuracy: 0.2976\n",
      "Epoch 4: val_accuracy improved from 0.27809 to 0.28790, saving model to models\\tok_lstm.h5\n",
      "1122/1122 [==============================] - 925s 824ms/step - loss: 2.0676 - accuracy: 0.2976 - val_loss: 2.0339 - val_accuracy: 0.2879\n",
      "Epoch 5/10\n",
      "1122/1122 [==============================] - ETA: 0s - loss: 1.9585 - accuracy: 0.3230\n",
      "Epoch 5: val_accuracy improved from 0.28790 to 0.34641, saving model to models\\tok_lstm.h5\n",
      "1122/1122 [==============================] - 915s 816ms/step - loss: 1.9585 - accuracy: 0.3230 - val_loss: 1.9050 - val_accuracy: 0.3464\n",
      "Epoch 6/10\n",
      "1122/1122 [==============================] - ETA: 0s - loss: 1.8770 - accuracy: 0.3553\n",
      "Epoch 6: val_accuracy improved from 0.34641 to 0.35990, saving model to models\\tok_lstm.h5\n",
      "1122/1122 [==============================] - 925s 825ms/step - loss: 1.8770 - accuracy: 0.3553 - val_loss: 1.8822 - val_accuracy: 0.3599\n",
      "Epoch 7/10\n",
      "1122/1122 [==============================] - ETA: 0s - loss: 1.7983 - accuracy: 0.3817\n",
      "Epoch 7: val_accuracy improved from 0.35990 to 0.36469, saving model to models\\tok_lstm.h5\n",
      "1122/1122 [==============================] - 932s 831ms/step - loss: 1.7983 - accuracy: 0.3817 - val_loss: 1.8684 - val_accuracy: 0.3647\n",
      "Epoch 8/10\n",
      "1122/1122 [==============================] - ETA: 0s - loss: 1.7108 - accuracy: 0.4039\n",
      "Epoch 8: val_accuracy did not improve from 0.36469\n",
      "1122/1122 [==============================] - 932s 831ms/step - loss: 1.7108 - accuracy: 0.4039 - val_loss: 1.8772 - val_accuracy: 0.3621\n",
      "Epoch 9/10\n",
      "1122/1122 [==============================] - ETA: 0s - loss: 1.6220 - accuracy: 0.4283\n",
      "Epoch 9: val_accuracy did not improve from 0.36469\n",
      "1122/1122 [==============================] - 932s 831ms/step - loss: 1.6220 - accuracy: 0.4283 - val_loss: 1.8833 - val_accuracy: 0.3498\n",
      "Epoch 10/10\n",
      "1122/1122 [==============================] - ETA: 0s - loss: 1.5224 - accuracy: 0.4598\n",
      "Epoch 10: val_accuracy did not improve from 0.36469\n",
      "1122/1122 [==============================] - 929s 828ms/step - loss: 1.5224 - accuracy: 0.4598 - val_loss: 1.9673 - val_accuracy: 0.3330\n",
      "Epoch 10: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21c856941c0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_lstm.fit(X_train_tok_lstm, y_train, validation_data=(X_test_tok_lstm, y_test), epochs=10, batch_size=32,\n",
    "             callbacks=[es_cb, getModelCheckpoint(\"tok_lstm\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 11\n",
    "\n",
    "tok_lstm_bi = tf.keras.Sequential([\n",
    "    Embedding(vocab_size, emb_dim, input_length=maxlen),\n",
    "    Bidirectional(LSTM(256, return_sequences=True, dropout=0.3), 'concat'),\n",
    "    Dropout(0.5),\n",
    "    LSTM(128, return_sequences=True, dropout=0.3),\n",
    "    Dropout(0.5),\n",
    "    LSTM(64, return_sequences=False, dropout=0.3),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# chosen_opt = tf.keras.optimizers.Adam(1e-4)\n",
    "chosen_opt ='adam'\n",
    "tok_lstm_bi.compile(loss='sparse_categorical_crossentropy', optimizer=chosen_opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1122/1122 [==============================] - ETA: 0s - loss: 2.1771 - accuracy: 0.2763\n",
      "Epoch 1: val_accuracy improved from -inf to 0.28890, saving model to models\\tok_lstm_bi.h5\n",
      "1122/1122 [==============================] - 1400s 1s/step - loss: 2.1771 - accuracy: 0.2763 - val_loss: 2.1193 - val_accuracy: 0.2889\n",
      "Epoch 2/10\n",
      "1122/1122 [==============================] - ETA: 0s - loss: 2.0552 - accuracy: 0.3090\n",
      "Epoch 2: val_accuracy improved from 0.28890 to 0.31765, saving model to models\\tok_lstm_bi.h5\n",
      "1122/1122 [==============================] - 1426s 1s/step - loss: 2.0552 - accuracy: 0.3090 - val_loss: 1.9744 - val_accuracy: 0.3177\n",
      "Epoch 3/10\n",
      "1122/1122 [==============================] - ETA: 0s - loss: 1.9823 - accuracy: 0.3249\n",
      "Epoch 3: val_accuracy improved from 0.31765 to 0.31799, saving model to models\\tok_lstm_bi.h5\n",
      "1122/1122 [==============================] - 1458s 1s/step - loss: 1.9823 - accuracy: 0.3249 - val_loss: 2.0249 - val_accuracy: 0.3180\n",
      "Epoch 4/10\n",
      "1122/1122 [==============================] - ETA: 0s - loss: 1.8879 - accuracy: 0.3579\n",
      "Epoch 4: val_accuracy improved from 0.31799 to 0.34318, saving model to models\\tok_lstm_bi.h5\n",
      "1122/1122 [==============================] - 1450s 1s/step - loss: 1.8879 - accuracy: 0.3579 - val_loss: 1.9318 - val_accuracy: 0.3432\n",
      "Epoch 5/10\n",
      "1122/1122 [==============================] - ETA: 0s - loss: 1.9840 - accuracy: 0.3276\n",
      "Epoch 5: val_accuracy did not improve from 0.34318\n",
      "1122/1122 [==============================] - 1419s 1s/step - loss: 1.9840 - accuracy: 0.3276 - val_loss: 2.0226 - val_accuracy: 0.3051\n",
      "Epoch 6/10\n",
      "1122/1122 [==============================] - ETA: 0s - loss: 1.8211 - accuracy: 0.3777\n",
      "Epoch 6: val_accuracy improved from 0.34318 to 0.35354, saving model to models\\tok_lstm_bi.h5\n",
      "1122/1122 [==============================] - 1375s 1s/step - loss: 1.8211 - accuracy: 0.3777 - val_loss: 1.9061 - val_accuracy: 0.3535\n",
      "Epoch 7/10\n",
      "1122/1122 [==============================] - ETA: 0s - loss: 1.7205 - accuracy: 0.4045\n",
      "Epoch 7: val_accuracy did not improve from 0.35354\n",
      "1122/1122 [==============================] - 1430s 1s/step - loss: 1.7205 - accuracy: 0.4045 - val_loss: 1.9405 - val_accuracy: 0.3475\n",
      "Epoch 8/10\n",
      "1122/1122 [==============================] - ETA: 0s - loss: 1.6352 - accuracy: 0.4293\n",
      "Epoch 8: val_accuracy did not improve from 0.35354\n",
      "1122/1122 [==============================] - 1420s 1s/step - loss: 1.6352 - accuracy: 0.4293 - val_loss: 1.9367 - val_accuracy: 0.3449\n",
      "Epoch 9/10\n",
      "1122/1122 [==============================] - ETA: 0s - loss: 1.5572 - accuracy: 0.4504\n",
      "Epoch 9: val_accuracy did not improve from 0.35354\n",
      "1122/1122 [==============================] - 1454s 1s/step - loss: 1.5572 - accuracy: 0.4504 - val_loss: 2.0526 - val_accuracy: 0.3486\n",
      "Epoch 9: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21c8573ac10>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_lstm_bi.fit(X_train_tok_lstm, y_train, validation_data=(X_test_tok_lstm, y_test), epochs=10, batch_size=32,\n",
    "             callbacks=[es_cb, getModelCheckpoint(\"tok_lstm_bi\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
